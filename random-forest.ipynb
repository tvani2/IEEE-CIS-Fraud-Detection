{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:13:30.884221Z","iopub.execute_input":"2025-04-29T14:13:30.884428Z","iopub.status.idle":"2025-04-29T14:13:31.276978Z","shell.execute_reply.started":"2025-04-29T14:13:30.884409Z","shell.execute_reply":"2025-04-29T14:13:31.276051Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install mlflow dagshub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='tvani2', repo_name='IEEE-CIS-Fraud-Detection', mlflow=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cleaning and engineering","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\n\nclass FullPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 target_column='isFraud',\n                 transaction_thresh=0.6,\n                 identity_thresh=0.9,\n                 identity_df=None):\n        self.target_column = target_column\n        self.transaction_thresh = transaction_thresh\n        self.identity_thresh = identity_thresh\n        self.identity_df = identity_df  # identity will be passed during initialization\n\n    def fit(self, X, y=None):\n        # 1. Drop columns with too many missing values\n        self.transaction_cols_to_keep = X.columns[X.isnull().mean() < self.transaction_thresh].tolist()\n        if self.identity_df is not None:\n            self.identity_cols_to_keep = self.identity_df.columns[self.identity_df.isnull().mean() < self.identity_thresh].tolist()\n        else:\n            self.identity_cols_to_keep = []\n\n        # 2. Merge\n        if self.identity_df is not None:\n            identity_filtered = self.identity_df[self.identity_cols_to_keep]\n            X = X[self.transaction_cols_to_keep].merge(identity_filtered, how='left', on='TransactionID')\n        else:\n            X = X[self.transaction_cols_to_keep]\n\n        # 3. Separate numeric and categorical columns\n        self.numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n        self.categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n        # 4. Imputers\n        self.numeric_imputer = SimpleImputer(strategy='mean')\n        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n\n        # Fit imputers\n        self.numeric_imputer.fit(X[self.numeric_cols])\n        self.categorical_imputer.fit(X[self.categorical_cols])\n\n        # 5. Determine WOE and one-hot columns\n        s = X[self.categorical_cols].nunique()\n        self.woe_columns = list(s[s > 3].index)\n        self.one_hot_columns = list(s[s <= 3].index)\n\n        # 6. Fit WOE mappings\n        if y is not None:\n            df_woe = X[self.woe_columns].copy()\n            df_woe['target'] = y.reset_index(drop=True)\n\n            self.woe_mappings = {}\n            self.woe_columns_fillna = df_woe[self.woe_columns].mode().T[0].to_dict()\n\n            for col in self.woe_columns:\n                groups = df_woe.groupby(col)['target'].agg(['count', 'mean'])\n                groups['n_pos'] = groups['mean'] * groups['count']\n                groups['n_neg'] = groups['count'] - groups['n_pos']\n\n                total_pos = groups['n_pos'].sum()\n                total_neg = groups['n_neg'].sum()\n\n                groups['prop_pos'] = groups['n_pos'] / total_pos\n                groups['prop_neg'] = groups['n_neg'] / total_neg\n\n                groups['woe'] = np.log(groups['prop_pos'] / groups['prop_neg'])\n\n                groups.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n                self.woe_mappings[col] = groups['woe'].to_dict()\n\n        return self\n\n    def transform(self, X):\n        # 1. Drop columns with too many missing values\n        if self.identity_df is not None:\n            identity_filtered = self.identity_df[self.identity_cols_to_keep]\n            X = X[self.transaction_cols_to_keep].merge(identity_filtered, how='left', on='TransactionID')\n        else:\n            X = X[self.transaction_cols_to_keep]\n\n        # 2. Impute missing values\n        X[self.numeric_cols] = self.numeric_imputer.transform(X[self.numeric_cols])\n        X[self.categorical_cols] = self.categorical_imputer.transform(X[self.categorical_cols])\n\n        # 3. Apply WOE encoding\n        for col in self.woe_columns:\n            new_col = f'{col}_woe'\n            X[new_col] = (\n                X[col]\n                .map(self.woe_mappings[col])\n                .fillna(self.woe_mappings[col].get(self.woe_columns_fillna[col], 0))\n            )\n\n        # 4. One-hot encode\n        X = pd.get_dummies(X, columns=self.one_hot_columns, drop_first=True, dummy_na=True)\n\n        # 5. Drop original WOE and one-hot columns\n        cols_to_drop = [col for col in (self.woe_columns + self.one_hot_columns) if col in X.columns]\n        X = X.drop(columns=cols_to_drop)\n\n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Load data\ntransaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\nidentity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\n\n# Separate target\ntarget_column = 'isFraud'\ny = transaction[target_column]\nX = transaction.drop(columns=[target_column])\n\n# Initialize preprocessor\npreprocessor = FullPreprocessor(\n    target_column=target_column,\n    transaction_thresh=0.6,\n    identity_thresh=0.9,\n    identity_df=identity\n)\n\n# Fit-transform\nX_processed = preprocessor.fit_transform(X, y)\n\nprint(X_processed.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature selection","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport pandas as pd\n\nclass CorrelationDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.9):\n        self.threshold = threshold\n        self.to_drop_ = None\n\n    def fit(self, X, y=None):\n        # 1. Calculate correlation matrix\n        corr_matrix = X.corr().abs()\n        \n        # 2. Upper triangle of the correlation matrix\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        \n        # 3. Find features with correlation greater than threshold\n        self.to_drop_ = [column for column in upper.columns if any(upper[column] > self.threshold)]\n        \n        print(f\"Columns to drop due to high correlation ({len(self.to_drop_)}): {self.to_drop_}\")\n        \n        return self\n\n    def transform(self, X):\n        # 4. Drop them\n        X_dropped = X.drop(columns=self.to_drop_, errors='ignore')\n        return X_dropped\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dropper = CorrelationDropper(threshold=0.9)\n\n# Fit on training data and transform it\nX_train_new = dropper.fit_transform(X_train)\n\n# Only transform test data\nX_test_new = dropper.transform(X_test)\n\n# Done!\nprint(\"Train set:\", X_train_new.shape)\nprint(\"Test set:\", X_test_new.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndef build_random_forest_pipeline():\n    \"\"\"\n    Faster Random Forest pipeline with minimal GridSearchCV.\n    \"\"\"\n    pipeline = Pipeline([\n        ('model', RandomForestClassifier(random_state=42, n_jobs=-1))  # Removed class_weight\n    ])\n\n    # Minimal hyperparameter grid\n    param_grid = {\n        'model__n_estimators': [30],                # Only 1 value\n        'model__max_depth': [3, 7],                 # Keep shallow trees\n        'model__max_features': ['sqrt']             # Only sqrt (faster)\n    }\n\n    # Faster GridSearch (2-fold CV)\n    model = GridSearchCV(\n        pipeline,\n        param_grid=param_grid,\n        scoring='roc_auc',\n        cv=2,                                      # Fewer folds\n        verbose=1,\n        n_jobs=-1\n    )\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Build the model\nmodel = build_random_forest_pipeline()\n\n# 2. Train the model\nmodel.fit(X_train, y_train)\n\n# 3. Predict\ntrain_preds = model.predict(X_train)\ntest_preds = model.predict(X_test)\n\ntrain_probs = model.predict_proba(X_train)[:, 1]\ntest_probs = model.predict_proba(X_test)[:, 1]\n\n# 4. Evaluate\nfrom sklearn.metrics import roc_auc_score, classification_report\n\nprint(\"Best parameters found:\", model.best_params_)\n\nprint(\"Train ROC-AUC:\", roc_auc_score(y_train, train_probs))\nprint(\"Test ROC-AUC:\", roc_auc_score(y_test, test_probs))\n\nprint(\"\\nTrain classification report:\\n\", classification_report(y_train, train_preds))\nprint(\"\\nTest classification report:\\n\", classification_report(y_test, test_preds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nimport mlflow.sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n\ndef build_random_forest_pipeline():\n    \"\"\"\n    Faster Random Forest pipeline with minimal GridSearchCV.\n    \"\"\"\n    pipeline = Pipeline([\n        ('model', RandomForestClassifier(random_state=42, n_jobs=-1))\n    ])\n\n    # Minimal hyperparameter grid\n    param_grid = {\n        'model__n_estimators': [30],\n        'model__max_depth': [3, 7],\n        'model__max_features': ['sqrt']\n    }\n\n    model = GridSearchCV(\n        pipeline,\n        param_grid=param_grid,\n        scoring='roc_auc',\n        cv=2,\n        verbose=1,\n        n_jobs=-1\n    )\n\n    return model\n\n# 1. Build the model\nmodel = build_random_forest_pipeline()\n\n# 2. Start MLflow run\nmlflow.set_experiment(\"Random_Forest_Training\")\n\nwith mlflow.start_run(run_name=\"Random_Forest_Run\"):\n    # 3. Train the model\n    model.fit(X_train, y_train)\n\n    # 4. Predict\n    train_preds = model.predict(X_train)\n    test_preds = model.predict(X_test)\n\n    train_probs = model.predict_proba(X_train)[:, 1]\n    test_probs = model.predict_proba(X_test)[:, 1]\n\n    # 5. Evaluate\n    train_roc_auc = roc_auc_score(y_train, train_probs)\n    test_roc_auc = roc_auc_score(y_test, test_probs)\n\n    print(\"Best parameters found:\", model.best_params_)\n\n    print(\"Train ROC-AUC:\", train_roc_auc)\n    print(\"Test ROC-AUC:\", test_roc_auc)\n\n    print(\"\\nTrain classification report:\\n\", classification_report(y_train, train_preds))\n    print(\"\\nTest classification report:\\n\", classification_report(y_test, test_preds))\n\n    # 6. Log params, metrics, and model\n    mlflow.log_params(model.best_params_)\n    mlflow.log_metric(\"train_roc_auc\", train_roc_auc)\n    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n\n    # Optional: confusion matrix metrics\n    train_cm = confusion_matrix(y_train, train_preds)\n    test_cm = confusion_matrix(y_test, test_preds)\n    \n    mlflow.log_metric(\"train_TN\", train_cm[0, 0])\n    mlflow.log_metric(\"train_FP\", train_cm[0, 1])\n    mlflow.log_metric(\"train_FN\", train_cm[1, 0])\n    mlflow.log_metric(\"train_TP\", train_cm[1, 1])\n    \n    mlflow.log_metric(\"test_TN\", test_cm[0, 0])\n    mlflow.log_metric(\"test_FP\", test_cm[0, 1])\n    mlflow.log_metric(\"test_FN\", test_cm[1, 0])\n    mlflow.log_metric(\"test_TP\", test_cm[1, 1])\n\n    # Save the best model itself\n    mlflow.sklearn.log_model(model.best_estimator_, \"model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}