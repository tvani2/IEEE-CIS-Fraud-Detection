{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:43:13.721237Z","iopub.execute_input":"2025-04-29T14:43:13.721548Z","iopub.status.idle":"2025-04-29T14:43:15.768715Z","shell.execute_reply.started":"2025-04-29T14:43:13.721516Z","shell.execute_reply":"2025-04-29T14:43:15.767376Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install mlflow dagshub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:02:53.710569Z","iopub.execute_input":"2025-04-29T18:02:53.711924Z","iopub.status.idle":"2025-04-29T18:02:58.296478Z","shell.execute_reply.started":"2025-04-29T18:02:53.711883Z","shell.execute_reply":"2025-04-29T18:02:58.295168Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.22.0)\nRequirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.5.9)\nRequirement already satisfied: mlflow-skinny==2.22.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.22.0)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\nRequirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.1.8)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.50.0)\nRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.44)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.6.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.11.3)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (4.13.1)\nRequirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.34.2)\nRequirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\nRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (14.0.0)\nRequirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\nRequirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.0.0)\nRequirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.2)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\nRequirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.7.1)\nRequirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.2.3)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.37.29)\nRequirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\nRequirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.9)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.3.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.1.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (4.0.12)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.4)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: botocore<1.38.0,>=1.37.29 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.37.29)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.11.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.19.0)\nRequirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\nRequirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (2.27.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==2.22.0->mlflow) (0.46.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (5.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.2.18)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (75.1.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (0.37b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.2.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.17.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.6.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='tvani2', repo_name='IEEE-CIS-Fraud-Detection', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:03:05.668029Z","iopub.execute_input":"2025-04-29T18:03:05.669125Z","iopub.status.idle":"2025-04-29T18:03:05.839566Z","shell.execute_reply.started":"2025-04-29T18:03:05.669085Z","shell.execute_reply":"2025-04-29T18:03:05.838698Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"tvani2/IEEE-CIS-Fraud-Detection\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"tvani2/IEEE-CIS-Fraud-Detection\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository tvani2/IEEE-CIS-Fraud-Detection initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository tvani2/IEEE-CIS-Fraud-Detection initialized!\n</pre>\n"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\n\nclass FullPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 target_column='isFraud',\n                 transaction_thresh=0.6,\n                 identity_thresh=0.9,\n                 identity_df=None):\n        self.target_column = target_column\n        self.transaction_thresh = transaction_thresh\n        self.identity_thresh = identity_thresh\n        self.identity_df = identity_df  # identity will be passed during initialization\n\n    def fit(self, X, y=None):\n        # 1. Drop columns with too many missing values\n        self.transaction_cols_to_keep = X.columns[X.isnull().mean() < self.transaction_thresh].tolist()\n        if self.identity_df is not None:\n            self.identity_cols_to_keep = self.identity_df.columns[self.identity_df.isnull().mean() < self.identity_thresh].tolist()\n        else:\n            self.identity_cols_to_keep = []\n\n        # 2. Merge\n        if self.identity_df is not None:\n            identity_filtered = self.identity_df[self.identity_cols_to_keep]\n            X = X[self.transaction_cols_to_keep].merge(identity_filtered, how='left', on='TransactionID')\n        else:\n            X = X[self.transaction_cols_to_keep]\n\n        # 3. Separate numeric and categorical columns\n        self.numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n        self.categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n        # 4. Imputers\n        self.numeric_imputer = SimpleImputer(strategy='mean')\n        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n\n        # Fit imputers\n        self.numeric_imputer.fit(X[self.numeric_cols])\n        self.categorical_imputer.fit(X[self.categorical_cols])\n\n        # 5. Determine WOE and one-hot columns\n        s = X[self.categorical_cols].nunique()\n        self.woe_columns = list(s[s > 3].index)\n        self.one_hot_columns = list(s[s <= 3].index)\n\n        # 6. Fit WOE mappings\n        if y is not None:\n            df_woe = X[self.woe_columns].copy()\n            df_woe['target'] = y.reset_index(drop=True)\n\n            self.woe_mappings = {}\n            self.woe_columns_fillna = df_woe[self.woe_columns].mode().T[0].to_dict()\n\n            for col in self.woe_columns:\n                groups = df_woe.groupby(col)['target'].agg(['count', 'mean'])\n                groups['n_pos'] = groups['mean'] * groups['count']\n                groups['n_neg'] = groups['count'] - groups['n_pos']\n\n                total_pos = groups['n_pos'].sum()\n                total_neg = groups['n_neg'].sum()\n\n                groups['prop_pos'] = groups['n_pos'] / total_pos\n                groups['prop_neg'] = groups['n_neg'] / total_neg\n\n                groups['woe'] = np.log(groups['prop_pos'] / groups['prop_neg'])\n\n                groups.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n                self.woe_mappings[col] = groups['woe'].to_dict()\n\n        return self\n\n    def transform(self, X):\n        # 1. Drop columns with too many missing values\n        if self.identity_df is not None:\n            identity_filtered = self.identity_df[self.identity_cols_to_keep]\n            X = X[self.transaction_cols_to_keep].merge(identity_filtered, how='left', on='TransactionID')\n        else:\n            X = X[self.transaction_cols_to_keep]\n\n        # 2. Impute missing values\n        X[self.numeric_cols] = self.numeric_imputer.transform(X[self.numeric_cols])\n        X[self.categorical_cols] = self.categorical_imputer.transform(X[self.categorical_cols])\n\n        # 3. Apply WOE encoding\n        for col in self.woe_columns:\n            new_col = f'{col}_woe'\n            X[new_col] = (\n                X[col]\n                .map(self.woe_mappings[col])\n                .fillna(self.woe_mappings[col].get(self.woe_columns_fillna[col], 0))\n            )\n\n        # 4. One-hot encode\n        X = pd.get_dummies(X, columns=self.one_hot_columns, drop_first=True, dummy_na=True)\n\n        # 5. Drop original WOE and one-hot columns\n        cols_to_drop = [col for col in (self.woe_columns + self.one_hot_columns) if col in X.columns]\n        X = X.drop(columns=cols_to_drop)\n\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T17:33:22.759387Z","iopub.execute_input":"2025-04-29T17:33:22.760066Z","iopub.status.idle":"2025-04-29T17:33:22.785491Z","shell.execute_reply.started":"2025-04-29T17:33:22.760039Z","shell.execute_reply":"2025-04-29T17:33:22.784432Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Load data\ntransaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\nidentity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\n\n# Separate target\ntarget_column = 'isFraud'\ny = transaction[target_column]\nX = transaction.drop(columns=[target_column])\n\n# Initialize preprocessor\npreprocessor = FullPreprocessor(\n    target_column=target_column,\n    transaction_thresh=0.6,\n    identity_thresh=0.9,\n    identity_df=identity\n)\n\n# Fit-transform\nX_processed = preprocessor.fit_transform(X, y)\n\nprint(X_processed.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T17:33:27.996518Z","iopub.execute_input":"2025-04-29T17:33:27.996847Z","iopub.status.idle":"2025-04-29T17:34:16.914642Z","shell.execute_reply.started":"2025-04-29T17:33:27.996809Z","shell.execute_reply":"2025-04-29T17:34:16.913593Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n","output_type":"stream"},{"name":"stdout","text":"(590540, 277)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import mlflow\nimport mlflow.sklearn\n\n# 1. Set experiment\nmlflow.set_experiment('XGBoost_Training')\n\n# 2. Start the run\nwith mlflow.start_run(run_name=\"XGBoost_Cleaning\") as run:\n    # 3. Initialize and fit your preprocessor\n    preprocessor = FullPreprocessor(identity_df=identity)\n    preprocessor.fit(X, y)  # Only pass X and y (identity is now inside preprocessor)\n\n    # 4. Transform your data\n    X_processed = preprocessor.transform(X)\n\n    # 5. Log the preprocessor model\n    mlflow.sklearn.log_model(preprocessor, \"full_preprocessor\")\n\n    # 6. Optionally, log some metadata\n    mlflow.log_param(\"transaction_thresh\", preprocessor.transaction_thresh)\n    mlflow.log_param(\"identity_thresh\", preprocessor.identity_thresh)\n    mlflow.log_metric(\"num_features_after_cleaning\", X_processed.shape[1])\n\n    print(f\"Run ID: {run.info.run_id}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:55:23.141136Z","iopub.execute_input":"2025-04-29T15:55:23.141544Z","iopub.status.idle":"2025-04-29T15:55:25.337601Z","shell.execute_reply.started":"2025-04-29T15:55:23.141507Z","shell.execute_reply":"2025-04-29T15:55:25.336378Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# from sklearn.base import BaseEstimator, TransformerMixin\n# import numpy as np\n# import pandas as pd\n\n# class CorrelationDropper(BaseEstimator, TransformerMixin):\n#     def __init__(self, threshold=0.9):\n#         self.threshold = threshold\n#         self.to_drop_ = None\n\n#     def fit(self, X, y=None):\n#         # 1. Calculate correlation matrix\n#         corr_matrix = X.corr().abs()\n        \n#         # 2. Upper triangle of the correlation matrix\n#         upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        \n#         # 3. Find features with correlation greater than threshold\n#         self.to_drop_ = [column for column in upper.columns if any(upper[column] > self.threshold)]\n        \n#         print(f\"Columns to drop due to high correlation ({len(self.to_drop_)}): {self.to_drop_}\")\n        \n#         return self\n\n#     def transform(self, X):\n#         # 4. Drop them\n#         X_dropped = X.drop(columns=self.to_drop_, errors='ignore')\n#         return X_dropped\n\n#     def fit_transform(self, X, y=None):\n#         return self.fit(X, y).transform(X)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall scikit-learn imbalanced-learn -y\n!pip install scikit-learn imbalanced-learn --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:56:07.942230Z","iopub.execute_input":"2025-04-29T15:56:07.942581Z","iopub.status.idle":"2025-04-29T15:56:19.196017Z","shell.execute_reply.started":"2025-04-29T15:56:07.942558Z","shell.execute_reply":"2025-04-29T15:56:19.194679Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: imbalanced-learn 0.13.0\nUninstalling imbalanced-learn-0.13.0:\n  Successfully uninstalled imbalanced-learn-0.13.0\nCollecting scikit-learn\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting imbalanced-learn\n  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn) (2024.2.0)\nDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.13.0 scikit-learn-1.6.1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip uninstall scikit-learn imbalanced-learn -y\n!pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:57:07.169816Z","iopub.execute_input":"2025-04-29T15:57:07.170300Z","iopub.status.idle":"2025-04-29T15:57:15.908854Z","shell.execute_reply.started":"2025-04-29T15:57:07.170269Z","shell.execute_reply":"2025-04-29T15:57:15.907682Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-learn 1.6.1\nUninstalling scikit-learn-1.6.1:\n  Successfully uninstalled scikit-learn-1.6.1\nFound existing installation: imbalanced-learn 0.13.0\nUninstalling imbalanced-learn-0.13.0:\n  Successfully uninstalled imbalanced-learn-0.13.0\nCollecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.10.1\n  Downloading imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.10.1 scikit-learn-1.2.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# ====================== 1. CorrelationDropper ======================\nclass CorrelationDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.9):\n        self.threshold = threshold\n        self.to_drop_ = []\n        self.kept_features_ = []\n\n    def fit(self, X, y=None):\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_ = X.columns.tolist()\n            df = X\n        else:\n            self.feature_names_ = [f\"f{i}\" for i in range(X.shape[1])]\n            df = pd.DataFrame(X, columns=self.feature_names_)\n            \n        corr_matrix = df.corr().abs()\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        self.to_drop_ = [column for column in upper.columns if any(upper[column] > self.threshold)]\n        self.kept_features_ = [f for f in self.feature_names_ if f not in self.to_drop_]\n        print(f\"Dropped {len(self.to_drop_)}/{len(self.feature_names_)} features due to high correlation\")\n        return self\n\n    def transform(self, X):\n        if isinstance(X, pd.DataFrame):\n            return X[self.kept_features_]\n        else:\n            kept_indices = [i for i, f in enumerate(self.feature_names_) if f in self.kept_features_]\n            return X[:, kept_indices]\n\n# ====================== 2. XGBFeatureSelector ======================\nclass XGBFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=\"mean\", random_state=42):\n        self.threshold = threshold\n        self.random_state = random_state\n        self.feature_mask_ = None\n        self.selected_features_ = None\n\n    def fit(self, X, y):\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_ = X.columns.tolist()\n        else:\n            self.feature_names_ = [f\"f{i}\" for i in range(X.shape[1])]\n            \n        model = xgb.XGBClassifier(\n            objective='binary:logistic',\n            eval_metric='auc',\n            random_state=self.random_state,\n            use_label_encoder=False\n        )\n        model.fit(X, y)\n        \n        importances = model.feature_importances_\n        if self.threshold == \"mean\":\n            thresh_value = importances.mean()\n        elif isinstance(self.threshold, float):\n            thresh_value = self.threshold\n        else:\n            raise ValueError(\"Unsupported threshold value\")\n\n        self.feature_mask_ = importances >= thresh_value\n        self.selected_features_ = np.array(self.feature_names_)[self.feature_mask_]\n        print(f\"Selected {self.feature_mask_.sum()} / {len(importances)} features\")\n        return self\n\n    def transform(self, X):\n        if isinstance(X, pd.DataFrame):\n            return X[self.selected_features_]\n        return X[:, self.feature_mask_]\n\n# ====================== 3. CustomPipeline ======================\nclass CustomPipeline:\n    def __init__(self):\n        self.undersampler = RandomUnderSampler(random_state=42, sampling_strategy=0.2)\n        self.correlation_dropper = CorrelationDropper(threshold=0.9)\n        self.feature_selector = XGBFeatureSelector(threshold=\"mean\", random_state=42)\n        self.scaler = StandardScaler()\n        self.classifier = xgb.XGBClassifier(\n            objective='binary:logistic',\n            eval_metric='auc',\n            random_state=42,\n            use_label_encoder=False,\n            n_estimators=100,\n            max_depth=5,\n            tree_method='hist'\n        )\n\n    def fit(self, X, y):\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X)\n        \n        # Step 1: Undersample\n        X_resampled, y_resampled = self.undersampler.fit_resample(X, y)\n        \n        # Step 2: Drop correlated features\n        X_corr = self.correlation_dropper.fit_transform(X_resampled)\n        \n        # Step 3: Select important features\n        X_selected = self.feature_selector.fit_transform(X_corr, y_resampled)\n        \n        # Step 4: Scale features\n        X_scaled = self.scaler.fit_transform(X_selected)\n        \n        # Step 5: Train classifier\n        self.classifier.fit(X_scaled, y_resampled)\n        \n        # Store intermediate results for access\n        self.X_train_new_ = X_selected  # Or X_scaled if you want the final transformed version\n        self.y_train_new_ = y_resampled\n        \n        return self\n\n\n    def predict(self, X):\n        X_ready = self.transform(X)\n        return self.classifier.predict(X_ready)\n\n    def predict_proba(self, X):\n        X_ready = self.transform(X)\n        return self.classifier.predict_proba(X_ready)\n\n    def transform(self, X):\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X)\n            \n        X_corr = self.correlation_dropper.transform(X)\n        X_selected = self.feature_selector.transform(X_corr)\n        X_scaled = self.scaler.transform(X_selected)\n        return X_scaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:57:34.367486Z","iopub.execute_input":"2025-04-29T15:57:34.367962Z","iopub.status.idle":"2025-04-29T15:57:34.389236Z","shell.execute_reply.started":"2025-04-29T15:57:34.367929Z","shell.execute_reply":"2025-04-29T15:57:34.388224Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Create an instance of the pipeline\npipeline = CustomPipeline()\n\n# Train the pipeline on training data\npipeline.fit(X_train, y_train)\n\n# Predict using test data\ny_pred = pipeline.predict(X_test)\ny_proba = pipeline.predict_proba(X_test)[:, 1]\n\n# Evaluate\nprint(classification_report(y_test, y_pred))\nprint(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n\n# Access transformed training data (after undersampling, correlation drop, and feature selection)\nX_train_new = pipeline.X_train_new_\ny_train_new = pipeline.y_train_new_\n\n# Access transformed test data (processed using fitted pipeline)\nX_test_new = pipeline.transform(X_test)\ny_test_new = y_test.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:58:51.049908Z","iopub.execute_input":"2025-04-29T15:58:51.050293Z","iopub.status.idle":"2025-04-29T15:59:20.346507Z","shell.execute_reply.started":"2025-04-29T15:58:51.050268Z","shell.execute_reply":"2025-04-29T15:59:20.345416Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"Dropped 91/277 features due to high correlation\nSelected 30 / 186 features\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98    113866\n           1       0.56      0.55      0.56      4242\n\n    accuracy                           0.97    118108\n   macro avg       0.77      0.77      0.77    118108\nweighted avg       0.97      0.97      0.97    118108\n\nROC AUC: 0.9040297532705361\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# import mlflow\n# import mlflow.xgboost\n# from datetime import datetime\n# import pandas as pd\n# import numpy as np\n# import xgboost as xgb\n# from sklearn.base import BaseEstimator, TransformerMixin\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n# from imblearn.under_sampling import RandomUnderSampler\n\n# # Initialize MLflow\n# mlflow.set_experiment(\"XGBoost_Training1\")\n\n# # ====================== 1. CorrelationDropper ======================\n# class CorrelationDropper(BaseEstimator, TransformerMixin):\n#     def __init__(self, threshold=0.9):\n#         self.threshold = threshold\n#         self.to_drop_ = []\n#         self.kept_features_ = []\n\n#     def fit(self, X, y=None):\n#         if isinstance(X, pd.DataFrame):\n#             self.feature_names_ = X.columns.tolist()\n#             df = X\n#         else:\n#             self.feature_names_ = [f\"f{i}\" for i in range(X.shape[1])]\n#             df = pd.DataFrame(X, columns=self.feature_names_)\n            \n#         corr_matrix = df.corr().abs()\n#         upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n#         self.to_drop_ = [column for column in upper.columns if any(upper[column] > self.threshold)]\n#         self.kept_features_ = [f for f in self.feature_names_ if f not in self.to_drop_]\n#         print(f\"Dropped {len(self.to_drop_)}/{len(self.feature_names_)} features due to high correlation\")\n#         return self\n\n#     def transform(self, X):\n#         if isinstance(X, pd.DataFrame):\n#             return X[self.kept_features_]\n#         else:\n#             kept_indices = [i for i, f in enumerate(self.feature_names_) if f in self.kept_features_]\n#             return X[:, kept_indices]\n\n# # ====================== 2. XGBFeatureSelector ======================\n# class XGBFeatureSelector(BaseEstimator, TransformerMixin):\n#     def __init__(self, threshold=\"mean\", random_state=42):\n#         self.threshold = threshold\n#         self.random_state = random_state\n#         self.feature_mask_ = None\n#         self.selected_features_ = None\n\n#     def fit(self, X, y):\n#         if isinstance(X, pd.DataFrame):\n#             self.feature_names_ = X.columns.tolist()\n#         else:\n#             self.feature_names_ = [f\"f{i}\" for i in range(X.shape[1])]\n            \n#         model = xgb.XGBClassifier(\n#             objective='binary:logistic',\n#             eval_metric='auc',\n#             random_state=self.random_state,\n#             use_label_encoder=False\n#         )\n#         model.fit(X, y)\n        \n#         importances = model.feature_importances_\n#         if self.threshold == \"mean\":\n#             thresh_value = importances.mean()\n#         elif isinstance(self.threshold, float):\n#             thresh_value = self.threshold\n#         else:\n#             raise ValueError(\"Unsupported threshold value\")\n\n#         self.feature_mask_ = importances >= thresh_value\n#         self.selected_features_ = np.array(self.feature_names_)[self.feature_mask_]\n#         print(f\"Selected {self.feature_mask_.sum()} / {len(importances)} features\")\n#         return self\n\n#     def transform(self, X):\n#         if isinstance(X, pd.DataFrame):\n#             return X[self.selected_features_]\n#         return X[:, self.feature_mask_]\n\n# # ====================== 3. CustomPipeline ======================\n# class CustomPipeline:\n#     def __init__(self):\n#         self.undersampler = RandomUnderSampler(random_state=42, sampling_strategy=0.2)\n#         self.correlation_dropper = CorrelationDropper(threshold=0.9)\n#         self.feature_selector = XGBFeatureSelector(threshold=\"mean\", random_state=42)\n#         self.scaler = StandardScaler()\n#         self.classifier = xgb.XGBClassifier(\n#             objective='binary:logistic',\n#             eval_metric='auc',\n#             random_state=42,\n#             use_label_encoder=False,\n#             n_estimators=100,\n#             max_depth=5,\n#             tree_method='hist'\n#         )\n#         self.run_id = None\n\n#     def fit(self, X, y):\n#         with mlflow.start_run(run_name=\"XGBoost_Preprocessing\") as run:\n#             self.run_id = run.info.run_id\n            \n#             if not isinstance(X, pd.DataFrame):\n#                 X = pd.DataFrame(X)\n            \n#             # Log initial dataset stats\n#             mlflow.log_metric(\"initial_samples\", X.shape[0])\n#             mlflow.log_metric(\"initial_features\", X.shape[1])\n#             mlflow.log_metric(\"class_ratio\", np.mean(y))\n            \n#             # Step 1: Undersample\n#             X_resampled, y_resampled = self.undersampler.fit_resample(X, y)\n#             mlflow.log_metric(\"undersampled_samples\", X_resampled.shape[0])\n#             mlflow.log_metric(\"new_class_ratio\", np.mean(y_resampled))\n            \n#             # Step 2: Drop correlated features\n#             self.correlation_dropper.fit(X_resampled)\n#             X_corr = self.correlation_dropper.transform(X_resampled)\n#             mlflow.log_metric(\"features_after_correlation_drop\", X_corr.shape[1])\n#             mlflow.log_param(\"correlation_threshold\", self.correlation_dropper.threshold)\n#             mlflow.log_text(\"\\n\".join(self.correlation_dropper.to_drop_), \"dropped_features.txt\")\n            \n#             # Step 3: Select important features\n#             self.feature_selector.fit(X_corr, y_resampled)\n#             X_selected = self.feature_selector.transform(X_corr)\n#             mlflow.log_metric(\"final_features\", X_selected.shape[1])\n#             mlflow.log_text(\"\\n\".join(self.feature_selector.selected_features_), \"selected_features.txt\")\n            \n#             # Step 4: Scale features\n#             X_scaled = self.scaler.fit_transform(X_selected)\n            \n#             # Step 5: Train classifier\n#             self.classifier.fit(X_scaled, y_resampled)\n            \n#             # Store transformed data\n#             self.X_train_new_ = X_selected\n#             self.y_train_new_ = y_resampled\n            \n#             # Log model\n#             mlflow.xgboost.log_model(self.classifier, \"xgboost_model\")\n            \n#             return self\n\n#     def predict(self, X):\n#         X_ready = self.transform(X)\n#         return self.classifier.predict(X_ready)\n\n#     def predict_proba(self, X):\n#         X_ready = self.transform(X)\n#         return self.classifier.predict_proba(X_ready)\n\n#     def transform(self, X):\n#         if not isinstance(X, pd.DataFrame):\n#             X = pd.DataFrame(X)\n            \n#         X_corr = self.correlation_dropper.transform(X)\n#         X_selected = self.feature_selector.transform(X_corr)\n#         X_scaled = self.scaler.transform(X_selected)\n#         return X_scaled\n\n#     def evaluate(self, X_test, y_test):\n#         with mlflow.start_run(run_id=self.run_id):\n#             y_pred = self.predict(X_test)\n#             y_proba = self.predict_proba(X_test)[:, 1]\n            \n#             # Calculate metrics\n#             metrics = {\n#                 \"accuracy\": accuracy_score(y_test, y_pred),\n#                 \"roc_auc\": roc_auc_score(y_test, y_proba),\n#                 \"f1_score\": f1_score(y_test, y_pred),\n#                 \"precision\": precision_score(y_test, y_pred),\n#                 \"recall\": recall_score(y_test, y_pred)\n#             }\n            \n#             # Log metrics\n#             mlflow.log_metrics(metrics)\n            \n#             # Log classification report\n#             report = classification_report(y_test, y_pred, output_dict=True)\n#             mlflow.log_dict(report, \"classification_report.json\")\n            \n#             # Log feature importance plot\n#             import matplotlib.pyplot as plt\n#             fig, ax = plt.subplots(figsize=(10, 6))\n#             xgb.plot_importance(self.classifier, ax=ax)\n#             plt.tight_layout()\n#             mlflow.log_figure(fig, \"feature_importance.png\")\n#             plt.close()\n            \n#             return metrics\n\n# # ====================== 4. Example Usage ======================\n# if __name__ == \"__main__\":\n#     # Assuming you have X_train, y_train, X_test, y_test\n#     pipeline = CustomPipeline()\n    \n#     # Train and track\n#     pipeline.fit(X_train, y_train)\n    \n#     # Evaluate and log metrics\n#     metrics = pipeline.evaluate(X_test, y_test)\n    \n#     # Get transformed data\n#     X_train_new = pipeline.X_train_new_\n#     y_train_new = pipeline.y_train_new_\n    \n#     print(\"Training complete! Metrics:\")\n#     print(metrics)\n#     print(f\"\\nView results in MLflow UI with run ID: {pipeline.run_id}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DZVELIA ES KODI\n\n\n\n# import mlflow\n# import mlflow.sklearn\n\n# from sklearn.base import BaseEstimator, TransformerMixin\n# import numpy as np\n# import pandas as pd\n\n# # Your CorrelationDropper class\n# class CorrelationDropper(BaseEstimator, TransformerMixin):\n#     def __init__(self, threshold=0.9):\n#         self.threshold = threshold\n#         self.to_drop_ = None\n\n#     def fit(self, X, y=None):\n#         corr_matrix = X.corr().abs()\n#         upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n#         self.to_drop_ = [column for column in upper.columns if any(upper[column] > self.threshold)]\n#         print(f\"Columns to drop due to high correlation ({len(self.to_drop_)}): {self.to_drop_}\")\n#         return self\n\n#     def transform(self, X):\n#         X_dropped = X.drop(columns=self.to_drop_, errors='ignore')\n#         return X_dropped\n\n#     def fit_transform(self, X, y=None):\n#         return self.fit(X, y).transform(X)\n\n# # --- Now MLflow logging ---\n\n# # Start the experiment\n# mlflow.set_experiment(\"XGBoost_Training\")\n\n# with mlflow.start_run(run_name=\"XGBoost_Feature_Selection\") as run:\n#     dropper = CorrelationDropper(threshold=0.80)\n\n#     # Fit and transform train\n#     X_train_new = dropper.fit_transform(X_train)\n    \n#     # Transform test\n#     X_test_new = dropper.transform(X_test)\n\n#     # Log parameters\n#     mlflow.log_param(\"correlation_threshold\", dropper.threshold)\n#     mlflow.log_param(\"num_features_dropped\", len(dropper.to_drop_))\n    \n#     # Optionally log the dropped features list\n#     dropped_features_str = \",\".join(dropper.to_drop_)\n#     mlflow.log_text(dropped_features_str, \"dropped_features.txt\")\n\n#     # Log final shapes\n#     mlflow.log_param(\"X_train_shape\", X_train_new.shape)\n#     mlflow.log_param(\"X_test_shape\", X_test_new.shape)\n    \n#     print(\"Train set:\", X_train_new.shape)\n#     print(\"Test set:\", X_test_new.shape)\n\n# print(\"MLflow logging complete!\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\nimport xgboost as xgb\nimport numpy as np\n\n# Assume these exist already:\nX = pipeline.X_train_new_  # after undersampling + feature selection\ny = pipeline.y_train_new_\n\n# List of parameter combinations to try\nparam_grid = [\n    {\"n_estimators\": 100, \"max_depth\": 3, \"learning_rate\": 0.1},\n    {\"n_estimators\": 200, \"max_depth\": 5, \"learning_rate\": 0.05},\n    {\"n_estimators\": 300, \"max_depth\": 6, \"learning_rate\": 0.01},\n    {\"n_estimators\": 100, \"max_depth\": 4, \"learning_rate\": 0.2, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n    {\"n_estimators\": 150, \"max_depth\": 3, \"learning_rate\": 0.1, \"scale_pos_weight\": 5},  # account for imbalance\n]\n\n# K-Fold CV with stratification\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Try each param set\nfor i, params in enumerate(param_grid):\n    model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        use_label_encoder=False,\n        random_state=42,\n        tree_method='hist',\n        **params\n    )\n    \n    aucs = []\n    f1s = []\n    precisions = []\n    recalls = []\n    \n    for train_idx, val_idx in skf.split(X, y):\n        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n        model.fit(X_train_fold, y_train_fold)\n        y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n        y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n        \n        auc = roc_auc_score(y_val_fold, y_val_pred_proba)\n        f1 = f1_score(y_val_fold, y_val_pred)\n        precision = precision_score(y_val_fold, y_val_pred, zero_division=0)\n        recall = recall_score(y_val_fold, y_val_pred)\n        \n        aucs.append(auc)\n        f1s.append(f1)\n        precisions.append(precision)\n        recalls.append(recall)\n    \n    print(f\"\\nModel {i+1} with params: {params}\")\n    print(f\"Mean AUC: {np.mean(aucs):.4f}, Std AUC: {np.std(aucs):.4f}\")\n    print(f\"Mean F1: {np.mean(f1s):.4f}, Std F1: {np.std(f1s):.4f}\")\n    print(f\"Mean Precision: {np.mean(precisions):.4f}, Std Precision: {np.std(precisions):.4f}\")\n    print(f\"Mean Recall: {np.mean(recalls):.4f}, Std Recall: {np.std(recalls):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import Pipeline as ImbPipeline  \nfinal_pipeline = ImbPipeline(steps=[\n    ('preprocessor', FullPreprocessor(identity_df=identity)),\n    ('correlation_dropper', CorrelationDropper(threshold=0.9)),\n    ('feature_selector', XGBFeatureSelector(threshold=\"mean\", random_state=42)),\n    ('scaler', StandardScaler()),\n    ('classifier', xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        random_state=42,\n        use_label_encoder=False,\n        n_estimators=100,\n        max_depth=5,\n        tree_method='hist'\n    ))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nimport mlflow.xgboost\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\nimport xgboost as xgb\nimport numpy as np\n\n# Initialize MLflow experiment\nmlflow.set_experiment(\"XGBoost_Training1\")\n\n# Assume these exist already:\nX = pipeline.X_train_new_  # after undersampling + feature selection\ny = pipeline.y_train_new_\n\n# List of parameter combinations to try\nparam_grid = [\n    {\"n_estimators\": 100, \"max_depth\": 3, \"learning_rate\": 0.1},\n    {\"n_estimators\": 200, \"max_depth\": 5, \"learning_rate\": 0.05},\n    {\"n_estimators\": 300, \"max_depth\": 6, \"learning_rate\": 0.01},\n    {\"n_estimators\": 100, \"max_depth\": 4, \"learning_rate\": 0.2, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n    {\"n_estimators\": 150, \"max_depth\": 3, \"learning_rate\": 0.1, \"scale_pos_weight\": 5},\n]\n\n# Stratified K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, params in enumerate(param_grid):\n    with mlflow.start_run(run_name=f\"XGBoost_Training_{i+1}\"):\n        # Log hyperparameters\n        mlflow.log_params(params)\n\n        model = xgb.XGBClassifier(\n            objective='binary:logistic',\n            eval_metric='auc',\n            use_label_encoder=False,\n            random_state=42,\n            tree_method='hist',\n            **params\n        )\n\n        # Store fold-wise metrics\n        cv_metrics = {'auc': [], 'f1': [], 'precision': [], 'recall': []}\n\n        # Cross-validation\n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n            model.fit(X_train_fold, y_train_fold)\n            y_val_proba = model.predict_proba(X_val_fold)[:, 1]\n            y_val_pred = (y_val_proba > 0.5).astype(int)\n\n            # Calculate metrics\n            auc = roc_auc_score(y_val_fold, y_val_proba)\n            f1 = f1_score(y_val_fold, y_val_pred)\n            precision = precision_score(y_val_fold, y_val_pred, zero_division=0)\n            recall = recall_score(y_val_fold, y_val_pred)\n\n            # Log each fold's metrics\n            mlflow.log_metric(f'fold{fold}_auc', auc)\n            mlflow.log_metric(f'fold{fold}_f1', f1)\n            mlflow.log_metric(f'fold{fold}_precision', precision)\n            mlflow.log_metric(f'fold{fold}_recall', recall)\n\n            # Save for aggregation\n            cv_metrics['auc'].append(auc)\n            cv_metrics['f1'].append(f1)\n            cv_metrics['precision'].append(precision)\n            cv_metrics['recall'].append(recall)\n\n        # Log average and std of each metric\n        for metric in cv_metrics:\n            mean_val = np.mean(cv_metrics[metric])\n            std_val = np.std(cv_metrics[metric])\n            mlflow.log_metric(f'mean_{metric}', mean_val)\n            mlflow.log_metric(f'std_{metric}', std_val)\n\n        # Log model\n        mlflow.xgboost.log_model(model, artifact_path=\"model\")\n\n        # Console output\n        print(f\"\\nModel {i+1} with params: {params}\")\n        for metric in cv_metrics:\n            print(f\"Mean {metric.upper()}: {np.mean(cv_metrics[metric]):.4f}, \"\n                  f\"STD: {np.std(cv_metrics[metric]):.4f}\")\n\nprint(\"\\n✅ All experiments logged to MLflow with fold-wise and aggregated metrics.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\nimport xgboost as xgb\nimport numpy as np\nimport itertools\n\n# Grid of hyperparameters\nparam_grid = {\n    \"n_estimators\": [100, 200],\n    \"max_depth\": [3, 4, 5],\n    \"learning_rate\": [0.01, 0.05, 0.1],\n    \"scale_pos_weight\": [1, 5, 10],  # Try 1 if no imbalance\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.8]\n}\n\n# Generate all combinations\nall_params = list(itertools.product(\n    param_grid['n_estimators'],\n    param_grid['max_depth'],\n    param_grid['learning_rate'],\n    param_grid['scale_pos_weight'],\n    param_grid['subsample'],\n    param_grid['colsample_bytree']\n))\n\nbest_f1 = 0\nbest_config = None\n\n# K-Fold CV with stratification\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Loop through each combination\nfor i, (n, d, lr, spw, subs, colsample) in enumerate(all_params):\n    params = {\n        \"n_estimators\": n,\n        \"max_depth\": d,\n        \"learning_rate\": lr,\n        \"scale_pos_weight\": spw,\n        \"subsample\": subs,\n        \"colsample_bytree\": colsample\n    }\n\n    model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        use_label_encoder=False,\n        random_state=42,\n        tree_method='hist',\n        **params\n    )\n\n    aucs, f1s, precisions, recalls = [], [], [], []\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n        model.fit(X_train_fold, y_train_fold)\n        y_proba = model.predict_proba(X_val_fold)[:, 1]\n        y_pred = (y_proba > 0.5).astype(int)  # Can try 0.4/0.6 later\n\n        aucs.append(roc_auc_score(y_val_fold, y_proba))\n        f1s.append(f1_score(y_val_fold, y_pred))\n        precisions.append(precision_score(y_val_fold, y_pred, zero_division=0))\n        recalls.append(recall_score(y_val_fold, y_pred))\n\n    mean_f1 = np.mean(f1s)\n    if mean_f1 > best_f1:\n        best_f1 = mean_f1\n        best_config = params\n\n    print(f\"\\nModel {i+1} with params: {params}\")\n    print(f\"Mean AUC: {np.mean(aucs):.4f}, Std AUC: {np.std(aucs):.4f}\")\n    print(f\"Mean F1: {mean_f1:.4f}, Std F1: {np.std(f1s):.4f}\")\n    print(f\"Mean Precision: {np.mean(precisions):.4f}, Mean Recall: {np.mean(recalls):.4f}\")\n\nprint(\"\\n✅ Best config based on F1:\")\nprint(best_config)\nprint(f\"Best F1 Score: {best_f1:.4f}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    precision_recall_curve,\n    average_precision_score,\n    roc_auc_score,\n    f1_score,\n    precision_score,\n    recall_score\n)\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport numpy as np\n\n# 1. Auto-calculate scale_pos_weight\nscale_pos_weight = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\nprint(f\"Auto-calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n\n# 2. Set model parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': ['aucpr', 'auc'],  # prioritize aucpr\n    'tree_method': 'hist',\n    'random_state': 42,\n    'n_estimators': 300,\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'gamma': 0.1,\n    'scale_pos_weight': scale_pos_weight,\n    'reg_alpha': 0.1,\n    'reg_lambda': 1.0\n}\n\n# 3. Cross-validation\ncv_metrics = {'auc': [], 'aucpr': [], 'f1': [], 'precision': [], 'recall': []}\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor train_idx, val_idx in skf.split(X_train, y_train):\n    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    model = xgb.XGBClassifier(**params)\n    model.fit(\n        X_fold_train, y_fold_train,\n        eval_set=[(X_fold_val, y_fold_val)],\n        early_stopping_rounds=20,\n        verbose=False\n    )\n    \n    y_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n    y_pred = (y_pred_proba > 0.5).astype(int)\n    \n    cv_metrics['auc'].append(roc_auc_score(y_fold_val, y_pred_proba))\n    cv_metrics['aucpr'].append(average_precision_score(y_fold_val, y_pred_proba))\n    cv_metrics['f1'].append(f1_score(y_fold_val, y_pred))\n    cv_metrics['precision'].append(precision_score(y_fold_val, y_pred))\n    cv_metrics['recall'].append(recall_score(y_fold_val, y_pred))\n\n# 4. Print CV results\nprint(\"\\nCross-Validation Results:\")\nfor metric, values in cv_metrics.items():\n    print(f\"{metric.upper():<10} Mean: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n\n# 5. Train final model\nfinal_model = xgb.XGBClassifier(**params)\nfinal_model.fit(\n    X_train_new, y_train_new,\n    eval_set=[(X_test, y_test)],\n    early_stopping_rounds=20,\n    verbose=True\n)\n\n# 6. Evaluate on test set (default threshold)\ny_test_proba = final_model.predict_proba(X_test)[:, 1]\ny_test_pred = (y_test_proba > 0.5).astype(int)\n\nprint(\"\\nTest Set Performance (Default Threshold=0.5):\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_test_pred))\n\n# 7. Threshold tuning to improve recall\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_test_proba)\nf1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)\n\n# Find best threshold with recall ≥ 75%\ntry:\n    idx = np.where(recalls >= 0.75)[0][0]\n    optimal_threshold = thresholds[idx]\n    print(f\"\\nOptimal Threshold for Recall ≥ 75%: {optimal_threshold:.4f}\")\nexcept IndexError:\n    optimal_threshold = 0.5\n    print(\"\\nNo threshold found for Recall ≥ 75%. Using default 0.5.\")\n\n# 8. Evaluate with adjusted threshold\ny_test_adj = (y_test_proba >= optimal_threshold).astype(int)\nprint(\"\\nTest Set Performance (Adjusted Threshold):\")\nprint(classification_report(y_test, y_test_adj))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_test_adj))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nfrom imblearn.pipeline import Pipeline as ImbPipeline  # For using RandomUnderSampler inside pipeline\n\n# Final pipeline definition\nfinal_pipeline = ImbPipeline(steps=[\n    ('preprocessor', FullPreprocessor(identity_df=identity)),   # Accepts raw test set\n    ('undersample', RandomUnderSampler(random_state=42, sampling_strategy=0.2)),\n    ('correlation', CorrelationDropper(threshold=0.9)),\n    ('feature_selector', XGBFeatureSelector(threshold=\"mean\", random_state=42)),\n    ('scaler', StandardScaler()),\n    ('model', xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        random_state=42,\n        use_label_encoder=False,\n        n_estimators=100,\n        max_depth=5,\n        tree_method='hist'\n    ))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:02:04.041751Z","iopub.execute_input":"2025-04-29T18:02:04.043244Z","iopub.status.idle":"2025-04-29T18:02:04.059544Z","shell.execute_reply.started":"2025-04-29T18:02:04.043211Z","shell.execute_reply":"2025-04-29T18:02:04.058286Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import mlflow\nimport mlflow.xgboost\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\nimport xgboost as xgb\nimport numpy as np\n\nwith mlflow.start_run(run_name=\"XGBoost_FullPipeline\") as run:\n    final_pipeline.fit(X, y)  # Only X, not X_processed!\n    \n    mlflow.sklearn.log_model(final_pipeline, \"xgb_full_pipeline\")\n    mlflow.log_param(\"model\", \"XGBoost\")\n    mlflow.log_param(\"feature_selector\", \"SelectKBest\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T18:09:58.704614Z","iopub.execute_input":"2025-04-29T18:09:58.705042Z","iopub.status.idle":"2025-04-29T18:11:05.207061Z","shell.execute_reply.started":"2025-04-29T18:09:58.705012Z","shell.execute_reply":"2025-04-29T18:11:05.206096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/tmp/ipykernel_31/3894903214.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X[new_col] = (\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"Dropped 92/277 features due to high correlation\nSelected 36 / 185 features\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/29 18:11:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run XGBoost_FullPipeline at: https://dagshub.com/tvani2/IEEE-CIS-Fraud-Detection.mlflow/#/experiments/0/runs/1044c3529ec94fd19b53c5f3b53deb32\n🧪 View experiment at: https://dagshub.com/tvani2/IEEE-CIS-Fraud-Detection.mlflow/#/experiments/0\n","output_type":"stream"}],"execution_count":41}]}